---
layout: post
title: The CI/CD Codex: An Introduction to Common Terms
date: 2020-04-06
tags: cloud tutorial beginners aws
author: Matthew Tyler
image: 
---

# Introduction

Most of us are familiar with the process of building and running software; from writing our first few lines, to compiling, to seeing 'hello world' printed on a screen. Most professional software engineers will be familiar with various practices designed to help software engineers build and release software. Having said that, I've yet to see any two companies (or even a small group of engineers) have similar ideas on what constitutes the ideal way to automate their software delivery process. I'm not referring to things like language/framework choices, or even coding styles; instead I refer to the various practices that have sprung up around the software world in the last ten to twenty years. Pair Programming? BDD? Pull Requests? Version Control? CI? CD? non-prod? prod? staging? manual gates? etc. 

The tooling that developers use both in and around the release process is on that I find most interesting. It is one of the most critical stages in the software process, yet is regularly the most brittle, insecure, and misunderstood tools in most organizations. For those of you at large companies, ask around your office: just how many people /really/ understand your build tooling? It's not uncommon for their to be only one to be one or two build 'wizards' that really understand the companies release process end-to-end. This is a shame for a few reasons. It is the one place that all work flows through; having knowledge and an appreciatiation of the process means that more developers can contribute to it's improvement, thereby rapidly accelerating the productivity of not only themselves, but their peers as well. Likewise, a good build and release process confers a good knowledge of the frameworks and tools you are using. This is knowledge that can be employed to be a more productive programmer in your day-to-day.

I intend for this series to serve a few purposes;

- To serve as introduction to some common types of build & release processes

  It's entirely likely that you may spend an entire career and never release a library to public package repository. That said, having an understand of the process and challenges in doing so may help you in solving other problems. You may be struggling with to decide the best way to build and release your own projects, and I hope that you be able to derive some inspiration from the examples contained within.

- To allow the read to observe a build & release process as it increases in sophistication

  The education most receive in software delivery is one that comes via a trial-by-fire. You start with a small solution to a small problem and iterate from there. I think it was easy to do this in years gone by when times where simpler, but the fast-paced nature of the world has become fairly unforgiving, and there are fewer low-risk opportunities to really touch-on software release processes. Increasing levels of abstraction and the proliferation of managed services make it harder to gain context and to understand the 'why', which is neccesary when faced with obstacles that you may be unfamiliar with.

- To understand that software is distributed and consumed in different ways, and this inevitably influences the software development process

  Software is consumed in wildly different ways depending on the who it is for. Software is installed on desktops using binaries retrieved via FTP. Libraries are distributed via various package manager. SaaS is commonly delivered via web browser. How your software is delivered and consumed has a marked effect on your build & release process.

- Evaluate different CI/CD processes within the lens of the AWS Well-Architected Framework

  The automated release process that many developers use are almost as the software they are releasing. They are living applications themselves, even if many companies don't realise it. They have many of the same opportunities and threats to their existance as other applications - sometimes more so. The Well-Architect Framework identifies 5 Key Pillars: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization. We will evaluate our evolving release mechanisms against these pillars to understand their weaknesses and look for improvements.

Examples will primarily be within the AWS ecosystem, using AWS specific tooling. I have a fair few reasons for this, and chief among these is the popularity of AWS will likely make this relevant to a larger audience. Many are critical of the developer experience around AWS tooling (including myself at times), with complaints usually leveled at how byzantine the services are. For a guide like this though, the primitive nature of the services is an effective way to illustrate various boundaries between different stages in the process. AWS is fairly well known for releasing 'building blocks'; services that may offer an incomplete experience, but often serve as stepping stones to creating services that are more featurful or provide better abstractions; consider the evolution from EC2 Virtual Machines, to EC2 Container Service, to Fargate. We will consider the various release tooling within the AWS Code* suite in a similar light.

I don't intend for this to be the defacto guide to CI/CD mechanisms. I don't consider it to be source of truth for various definitions of practices in the community. I am sure there will be arguments for and against my particular understanding of different terms and practices. It's merely the understanding I've gained from working in and around CI/CD tooling over the last decade, and the conclusions I've drawn from that experience. This is weighted heavily to a create a working-persons guide to practical software delivery, rather than an expression of ideologic purity that is often at odds with the reality of the typical developers day-to-day.

# Definitions

Future installments will go into details of specific examples. Firstly we need to set some definitions and understand the main concepts that form the modern software delivery apparatus.

## (Distributed) Version Control Systems

Of anything in software development unrelated to the process of writing code, version control systems are probably the most impactful tools on a developers career. When I first began coding, I had no idea what a version control system was. When I later went to University during the mid to late 2000's, I was not introduced to version control until I had almost graduated. Up until then, my peers and I had gotten by the common practice of appending 'final' repeatedly to the end of the name of a zip file. I would not interact with a VCS until my first development job in around 2010, not begin using Git until two years later, and not become comfortable with it a few years after that.

At the time I was beginning my career there were several different systems vying for the market but it would seem Git has won the war. Millions of developers are using GitHub, of which Git is the defacto system of choice. Git itself has a long and storied history, created by and out of Linus Torvalds discontent with the licencing fees associated with the system used by the Linux Kernel at the time. As such, it retains a long list of features designed to work with the Linux Kernel development process that would seem completely foreign to the vast majority of the development community, most of whom are probably used to the kinds of workflows that are made easy by GitHub. I mean this as a comfort; if you are worried about the complexity of Git, stop worrying. None of us know how to use every bit of it either. Take the parts of Git you know as trusted ground, whilst viewing the parts you don't as an exciting frontier for further discovery at the right time. That attitude will take you further than fear.

Modern software would simply not function without Git or something like that. Maintaining a complete history of your code at certain points of time is the only way we can trace back what is running in our users hands to what we wrote. That link is critical to working out how to resolve issues. If the source code is the scene of the crime - version control is the video evidence of it.

Git is perhaps most importantly, an enabler of collaboration. Many developers can contribute to the development of software at the same time all on the same code base. This coordination of software changes ensures that software development is capable of scaling as the number of contributers increases. It is not perfect, but it will do.

## Branching Strategies

Branching strategies are the processes that teams use to indepedently work on features within a version control system. The two most popular are called Git Flow and Trunk Based Development.

### Git Flow

Git Flow uses multiple branchs that are designed to be active at any one point in time. These include;

- A development branch
- Multiple feature branchs
- Release branches
- The master (release) branch

Most development occurs by branching off of develop into a feature branch. At some point, development on the feature completes, at which point it is merged into the development branch. At some point, a release branch will be created. No changes except for bugfixes are made to the release branch. The release branch will be merged into master, with an appropriate tag added to signify the release. Hotfix branchs may be created from master for urgent production fixes, and are typically merge as fast as possible back into the master and develop branches.

<Git Flow Reference>
<Git Flow Image>

### Trunk Based Development

Trunk Based Development uses one long-lived branch - the 'trunk' or 'master' branch. All releases are made from this, typically in a rolling-release manner. Developers do their best to branch off of trunk for only a limited amount of time resulting in smaller changes in a more frequent cadence. This reduces the chances of multiple in-flight changes from clashing with each other, and also reduces the risk of rework and/or degredation. It additionally increases the likelihood that the head of the trunk will be in a healthy state, ready for release. Personally, I prefer trunk based development for the aforementioned reasons and believe it is the only way to get to fast release cycles through continuous delivery.

<Trunk based reference>
<Trunk image>

## Continuous Integration

It is important to establish whether the contributions of many developers are features or regressions. Simply allowing developers to contribute directly to a release without establishing whether existing code continues to work post-these-changes is generally considered by users of that software to be particularly important. But users also want new features as fast as possible, and they also want those features to work too. In the past limited resources and the lack of the tooling we are afforded today led to long and expensive manual testing cycles and slow releases, or software that released frequently but was constantly broken in various different ways.

Continuous Integration done right is a means to solve this problem. The 'common' implementation of continuous integration functions as thus;

1. The developer creates a set of changes to a code base, and creates a 'request' to merge her changes into the main code base.

2. As part of this request, an automatic process is kicked off. The process runs a suite of tests written both against the existing base and the new changes.

3. If these changes pass then the code is considered functionally healthy enough to be merged into the main code base, in effect becoming a part of it.

In the 'purest' form of continuous integration, there is no manual intervention by a human to review the code. Provided the test coverage of the entire code base doesn't drop, and the tests do not fail, that is enough for the system to automatically accept the new code. In practice there is usually  some manual approval required before accepting the code. Code reviews are usually less about the explicit functionality and more about checking that the style, convention and quality of the incoming code is of acceptable standard. Determining the values of the attributes cannot always be determined by automation, although technology is getting close to doing so. Code that is produce by pair-programming may not go through any kind manual approval or review - the act of writing code together should be enough quality control in most circumstances.

Continuous Integration gives a code base a particularly powerful property - that this code base could be released to a user at any time, and the software should work. You cannot have confidence this property will hold without a robust test suite that is executed on every change. Software that is in a shippable state, that is missing promised functionality, is still software that can be sold. That is still preferable to software that does not ship, and can't be sold.

In summation, Continuous Integration is an approach designed to ensure that software changes are accepted into the main software corpus as fast as possible, without a degradation in the overally quality of the code, and ensuring that the software could be released at any time. Other processes, like continuous deployment, and continuous delivery, cannot function with its presence. After version control I believe that establishing a strong continuous integration mechanism is the next element to focus on for a healthy software delivery process.

## Package Management

There are really two parts to package management; consuming packages, and releasing packages. The former is widely understood; you are probably very familiar with various package management systems, whether that be specific packaging hosting mechanisms like NPM (Node), Cargo (Rust), PyPi (Python), RPM (Red Hat) or some other mechanism like Go with GitHub. This had made it a lot easier to declare and use dependencies within your own code and provided a path to auditing them for vulnerabilities as well. This has made it easier for everyone to "build on the shoulders of giants". What is perhaps less understood is releasing packages.

Releasing packages can be difficult. It generally requires a good understanding of several things;

1. How package management works in your particular environment.
2. A good understanding of versioning; does this release contain breaking changes that will require intervention on the part of the consumer?
3. How is the package consumed? It is a service that needs to be launched? is it library code? is it a command line tool?
4. Does the target package need to be compiled for different processor architectures?

This is hardly an exhaustive list either.

Package management requires an understanding of whether your software even makes sense to distribute as a separate, reusable component. Despite 'DRY' (Don't Repeat Yourself) being a common software development mantra, the reality is that building a component that is useful enough to a wide audience is very difficult. It is far easier to build a component that ends up being only useful for a small number of cases, generally your own, because you know the context of your own situation far more intimately than that of someone else. This rubs up against that other software wisdom - YAGNI - or 'You Aren't Gonna Need It'. As a result of this, many software teams are hesitant to publish reusable libraries due to a double whammy of unfamiliarity with package publishing systems, and the (reasonably safe) assumption that nobody will need/want to use what they publish anyway.

Domain Driven Design proponents often espouse a technique where software is built reasonably monolithically to begin with, and later split into purpose built components. The thinking behind this is that developers working on the software will become more familiar with the domain as time goes on, and will therefore make better decisions about where the correct domain boundaries lie. This approach can be extended to any software component; you will likely be able to make better decisions regarding what code should be in a reusable library after messing about in the problem domain, rather than seperating it out front. Of course, if you have prior experience in the area and believe you can make this decision earlier, go for it! That is the value of an experienced senior engineer, after all.

## Continuous Delivery / Deployment

The other C in CI/CD, continuous delivery or deployment refers to the ability to release software at any point in time. Whereas continuous integration ensures that new code is able to be merged into the main line without introducing code-level defects, continuous deployment focuses on your ability to get those changes in front of users. There are generally two aspects to this which result in whether someone users the moniker of delivery or deployment.

1. Delivery - Whether the changes can be released to a customer at any point in time.
   
   The caveat here is that the release is functionally able to be released at any point in time. E.g. everything in mainline works 'as-is'; you are not waiting on future feature to make a previously integrated feature 'work'.

2. Deployment - As soon as a change is integrated into mainline, the code is released to end-users.

   This assumes the requirement for continuous delivery is met, and if we are doing that, we may as well deploy as often as possible.

Continuous Deployment is the goal, and ensures that software can be deployed to customers as fast as reasonably possible. Performing continuous deployment is a magnitude more difficult than continuous integration. It requires a myriad of additional techniques to ensure that code is ready to be released to users, and to roll-back changes in the event of a degredation to the application.

## Environments

While you can certainly do it, it is unusual to release changes directly into a production environment without them passing through some kind of 'test' environment first. The purpose of this is two-fold;

1. A test version of the system can be excercised as a form of pre-production testing, which allows us to build confidence in the changes we are about to release.

2. It excercise our deployment process; by deploying to the test environment we are, in a way, testing the deployment process that we will undertake against the production environment. Every deployment made to a test environment is in effect, testing the deployment itself. This does assume that your test deployment procedures and environments are as indentical as possible to it's production equivalent.

It's important to keep in mind that the only environment like production - is production. Test environments are never perfect replicas due to the ebs and flows and whims of traffic hitting your endpoints which can behave in strange and ever emergent ways. An increasing number of test environments beyond one or two tends to give diminishing returns. As with all testing, the intent is always to ask yourself - have I done enough, so that when it is time to deploy, can I be confident that this will work?

## Pipelines

A pipeline is a codified process that executes when a change has been made to a code repository. Pipelines can represent either CI or CD processes. 

A CI pipeline will generally include items that ensure the quality of a candidate change. Examples may include;

- Linting to ensure code meets stylistic standards
- Auditing of dependencies, both current and introduced, to check for vulnerabilities
- Other forms of status analysis, which could include checking for common coding errors or complexity analysis, etc.
- Executing tests of various forms (unit, integration, etc)
- Checking that test coverage does not decrease
- Compilation of the code
- A manual approval via code review before merge into mainline.

It is usually possible to run many of these checks in parallel. A failure may not nessecarily block a merge; these checks could just report metrics and leave the ultimate decision up to someone who is performing a code review.

<image of a CI process>

A CD pipeline will normally be triggered via merging of code into an existing branch; e.g. master for those that are using trunk-based development. This will generally result in;

- Compilation of the code
- Executing various forms of tests (unit, integration, etc).
- Deployment of the code to pre-production environment/s.
- Execution of end-to-end testing against pre-production environments.
- Supplementary testing e.g. load testing, penetration testing, etc.
- Phased roll-outs to live production environments via traffic shifting, canaries or blue-green deployments
- Synthetic traffic testing in production environment during low traffic periods.

Your CD pipeline will typically be less flexible than it's CI equivalent. It will usually mandate that each stage be passed successfully before proceeding to the next.

<image of a CD Process>

<Add additional terms>

## When Should I Introduce CI/CD Practices?

<Forshaw Twitter Question>

Ideally as early as you think it is reasonable to make the investment. That point is always reached the moment you have more than one developer working on a project. If you hacking along as a solo developer on a personal project, determining when you have reached that point is perhaps a little difficult. If you pushing all your commits to master because you are still determining whether there is any value to what you are building (or whether what you are trying out will even work), adding CI/CD might be a distraction. However, if you are personally invested in what you are building in the 'get rich or die trying' mindset, you may as well do CI/CD from the start. Similarly if you are using a test-first methodology like BDD or TDD, being able to run your tests as part of CI workflow make sense, so set it up.

Continuous Deployment is a little different - if you aren't releasing your software to anyone, it probably doesn't matter. That said, the nature of software is that when you need something, you need it in a hurry. Have a plan to work a little bit on creating a CD pipeline as your project progresses, and it will relieve a lot of stress when that time comes.

In summary - if your development is planned (and not experimental), it makes sense to start do CI from the start. CD is at your discretion.

<Conclusion>