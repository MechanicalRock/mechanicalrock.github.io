---
layout: post
title: Automating Serverless Streaming Data Analytics in AWS
date: 2020-09-01
tags: aws kinesis realtime analytics cloudformation
image: img/../.jpg
author: Marcelo Aravena
---
<center><img src="/img/kinesis-analytics/realize-real-time-analytics.jpg" /></center><br/>

Building data processing pipelines with streaming data in the cloud can be complex and expensive to run at scale, let alone implementing an efficient and manageable development process.  Today we will go through how to streamline your development and deployment of AWS Kinesis Analytics applications with DevOps best practices.  We will walk through how to build and deploy an application by using an automated CI/CD process, all around a serverless infrastructure, so taking away the need to manage servers and their administration, we just focus on our application code development with what data processing techniques we want to apply. 

### Why Analytics in Realtime? (actually near realtime when it comes to cloud integration)
- Real-time analytics allows businesses to get insights and act on data immediately or soon after the data enters their system, allowing you to run high performance analytics on streaming data which elastically scales to keep up with the volume of data being ingested. 
- Real time Analytics applicatons can answer queries or process(ETL) within seconds. ... For example; Financial Investors may want their own/custom analytics applied to live data being generated by the stock exchange, to help make informed trading decisions on whether to buy or sell a stock.  A scenrario we will use with the example below. 

### Kinesis Analytics Application Example: Ingesting/streaming live stock trading data into Kinesis Analytics 
The example application is one also used by AWS in their tutorials, will be ingesting stock trading data ('AMZN', 'GOOG', 'AZRE', 'ORCL', 'BABA') with their dollar values at a point in time, into a kinesis data stream which an Amazon Kinesis Data Analytics application will use a source to perform analytics on near-realtime.  We will be using a python script to simulate the generation and streaming of data which will randomly generate stock trade prices every second during the trading hours into a kinesis data stream endpoint.  The data processing application will be using Apache Flink as the Kinesis Analytics Runtime option.  With Apache Flink we will use a sliding time window of 1 minute to get the highest(max operator) price the stock was traded during that time window and output the results to another kinesis data stream.

#### Data Generator for stocks being traded
``` python
import sys
import json
import boto3
import random
import datetime
import time

kinesis = boto3.client('kinesis')

stocks = ['AMZN', 'GOOG', 'AZRE', 'ORCL', 'BABA']
stream = 'stocktrading-ingest-stream'

def genData(cloudPlatform):
    data = {}
    now = datetime.datetime.now()
    str_now = now.isoformat()
    data['EVENT_TIME'] = str_now
    data['TICKER'] =  random.choice(stocks)
    price = random.random() * 100
    data['PRICE'] = round(price, 2)
    return data

tradingVolume = 0
while tradingVolume <= 100000:
    for cloudPlatform in stocks:
        data = json.dumps(genData(cloudPlatform))
        kinesis.put_record(
            StreamName=stream,
            Data=data,
            PartitionKey="partitionkey")
    time.sleep(1)
    tradingVolume += 1
```

## AWS Cloud Technology Stack and Environment
Amazon Kinesis Data Analytics is serverless, there are no servers to manage and no minumum fee or setup costs, just the resources the application uses when its running. It runs your streaming applications without requiring you to provision or manage any infrastructure. Amazon Kinesis Data Analytics automatically scales the infrastructure up and down as required to run your applications with low latency.

Everything is written and defined as code. Cloudformation being the Infrastructure as Code tool, along with GitHub, Codepipeline and CodeBuild to make up the CI/CD components.  The Kinesis Analytics Apache Flink Java application will then be compiled with the jar artifact published to an s3 bucket which is where Kinesis Analytics launches the Flink Java application from. Any updates to the applications pushed to the Github repo will trigger a new build and publish to S3, which Kinesis analytics will apply as an update to your streaming data-pipeline. 

The Java Flink application calls environment variables such as inputStreamName, outputStreamName, AWS Region and other kinesis properties, which are defined in the kinesis analytics cloudformation template(Under Environment Properties).  This is so that any changes that we meed to make to the environment, we dont need to re-build the Java application source, we just update the cloudformation template which the application makes reference of. 

## Performance, Throughput and Scaling capabilities of AWS Kinesis Services used
### Kinesis Data Streams 
Max size of Payload = 1MB
- 1 Shard = 1MB/s Input, 1000 Puts/Sec. 2MB/s Output.
- 2 Shards = 2 MB/s Input, 2000 Puts/Sec. 4MB/s Output.

### Kinesis Firehose Delivery Stream:
- Autoscales the input from producers, no need to workout shards or throughput. A source for Kinesis Firehose can also be a Kinesis Data Stream
- Buffering configurable by Kinesis Firehose.  Buffer Size and Buffer time(How long you want to keep the buffer in before persisting it)
- Transformations can be performed with Kinesis Firehose and Lambda before persisting to your data store/lake/warehouse. This method is good if your transformations are simple or you have small scale data-processing requirements which would be overkill for Kinesis Analytics (Apache Flink or SQL).

### Kinesis Analytics
Kinesis Analytics is a serverless platform to analyze streaming data with on the fly ETL and data processing operations near real-time! Giving your data pipelines the legs it needs without having to worry about how many legs you need to provide it while also reducing the complexity that most other data pipelines come with. 

When taking the serverless journey with kinesis analytics, like many other serverless or managed service products on AWS you should have a good idea on your data throughput, storage and performance requirements to help you manage costs and scaling.  Kinesis Data Anaylytics provisions capacity as Kinesis Processing Units (KPU), where 1 KPU is equal to 1vCPU and 4GB Memory/RAM. For Apache flink applications where 1 KPU is used, you will get 50GB of storage to run your application which includes the use of checkpoints(having backups of the application) and temporary scratch storage you may want to build into your streaming ETL/Data Processing application.

## Kinesis Analytics CI/CD Process
<center><img src="/img/kinesis-analytics/KinesisAnalytics-CICD.png" /></center><br/>

Application Source and Infrastructure as Code templates for your reference can be found at the following Mechanical Rock Github repo: [Kinesis Analytics Application and Infrastructure Repo](https://github.com/MechanicalRock/kinesis-analytics)


## AWS Kinesis Analytics Architecture

<center><img src="/img/kinesis-analytics/KinesisAnalyticsArch.png" /></center><br/>

The above Architecture diagram for streaming data analytics in AWS shows how flexible we can be with data sources (data producers), data consumers and how we can persist data in AWS.  Having the "processed" data output to another Kinesis Data stream gives you the option of adding additional consumers down the track for new requirements, as well as piping it to Kinesis Firehose to persist the data into a Redshift, S3, ElasticSearch, HTTP Endpoints, Datadog, Splunk and more, without having to change the core archicture of how you ingest the data and perform near realtime analytics. 

You now have more time to work on improving or enhancing your streaming analytics rather than having to wrangle server infrastructure to process your streaming jobs efficiently and scale without too much as lifting a single cpu or memory chip. 

